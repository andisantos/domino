{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5482c273",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Notebook workspace for PlaneSpotSlicer.\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "* install scvis using this package: https://github.com/shahcompbio/scvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "532aa14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import meerkat as mk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from domino.utils import unpack_args\n",
    "\n",
    "from abstract import Slicer\n",
    "from sklearn import mixture\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46ced952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import meerkat as mk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from domino.utils import unpack_args\n",
    "\n",
    "from abstract import Slicer\n",
    "\n",
    "## PlaneSpot imports\n",
    "from sklearn import mixture\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "from domino.utils import convert_to_numpy, unpack_args\n",
    "import pandas as pd\n",
    "\n",
    "class PlaneSpotSlicer(Slicer):\n",
    "    r\"\"\"\n",
    "    Implements PlaneSpot [plumb_2023], a simple SDM that fits a GMM to a 2D model \n",
    "    embedding, fit using scvis [ding_2018]. \n",
    "\n",
    "    ..  [plumb_2023]\n",
    "        Gregory Plumb*, Nari Johnson*, Ángel Alexander Cabrera, Ameet Talwalkar.\n",
    "        Towards a More Rigorous Science of Blindspot Discovery in Image \n",
    "        Classification Models. arXiv:2207.04104 [cs] (2023)\n",
    "        \n",
    "    ..  [ding_2018]\n",
    "        Jiarui Ding, Anne Condon, and Sohrab P Shah. \n",
    "        Interpretable dimensionality reduction of single cell transcriptome \n",
    "        data with deep generative models. \n",
    "        Nature communications, 9(1):1–13. (2018)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        scvis_conda_env: str, # name of conda environment where scvis is installed\n",
    "        n_slices: int = 10,\n",
    "        n_max_mixture_components: int = 33, # maximum number of mixture components\n",
    "        weight: float = 0.025, # weight hyperparameter\n",
    "        scvis_config_path = None, # custom scvis config path\n",
    "        scvis_output_dir = 'scvis', # path to output directory for scvis\n",
    "        fit_scvis = True # flag to load rather than re-compute the scvis embedding \n",
    "    ):\n",
    "        super().__init__(n_slices=n_slices)\n",
    "        \n",
    "        # scvis hyper-parameters\n",
    "        self.scvis_conda_env = scvis_conda_env\n",
    "        self.config.scvis_config_path = scvis_config_path\n",
    "        self.config.scvis_output_dir = scvis_output_dir\n",
    "        self.fit_scvis = fit_scvis\n",
    "        \n",
    "        # GMM hyper-parameters\n",
    "        self.config.n_max_mixture_components = n_max_mixture_components\n",
    "        self.config.weight = weight\n",
    "\n",
    "        self.gmm = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: Union[dict, mk.DataPanel] = None,\n",
    "        embeddings: Union[str, np.ndarray] = \"embedding\",\n",
    "        targets: Union[str, np.ndarray] = None,\n",
    "        pred_probs: Union[str, np.ndarray] = None,\n",
    "        losses: Union[str, np.ndarray] = None,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        embeddings, targets, pred_probs, losses = unpack_args(\n",
    "            data, embeddings, targets, pred_probs, losses\n",
    "        )\n",
    "        \n",
    "        embeddings, targets, pred_probs = convert_to_numpy(\n",
    "            embeddings, targets, pred_probs\n",
    "        )\n",
    "        \n",
    "        # 1.  Fit scvis.\n",
    "        if verbose:\n",
    "            print('Fitting scvis...')\n",
    "        \n",
    "        scvis_embeddings = self._fit_scvis(embeddings.reshape(embeddings.shape[0], embeddings.shape[1]))\n",
    "        \n",
    "        # 2.  Fit GMM.\n",
    "        if verbose:\n",
    "            print('Fitting GMM...')\n",
    "            \n",
    "        self._fit_gmm(scvis_embeddings,\n",
    "                     pred_probs,\n",
    "                     verbose)\n",
    "\n",
    "    def predict_proba(\n",
    "        self,\n",
    "        data: mk.DataPanel,\n",
    "        scvis_embeddings: str, # scvis column name\n",
    "        pred_probs: str, # predicted probabilities column name\n",
    "    ) -> np.ndarray:\n",
    "        \n",
    "        # Append the scvis embedding and predicted probabilities; normalize\n",
    "        X = self._combine_embedding(dp[scvis_embeddings], dp[pred_probs])\n",
    "        return self.gmm.predict_proba(X)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        data: mk.DataPanel,\n",
    "        embeddings: Union[str, np.ndarray] = \"embedding\",\n",
    "        targets: Union[str, np.ndarray] = None,\n",
    "        pred_probs: Union[str, np.ndarray] = None,\n",
    "        losses: Union[str, np.ndarray] = None,\n",
    "    ) -> np.ndarray:\n",
    "        probs = self.predict_proba(\n",
    "            data=data,\n",
    "            embeddings=embeddings,\n",
    "            targets=targets,\n",
    "            pred_probs=pred_probs,\n",
    "            losses=losses,\n",
    "        )\n",
    "\n",
    "        # TODO (Greg): check if this is the preferred way to get hard predictions from\n",
    "        # probabilities\n",
    "        return (probs > 0.5).astype(np.int32)\n",
    "    \n",
    "    def _load_scvis_embeddings(self) -> np.ndarray:\n",
    "        ''' Load and return the scvis embeddings.\n",
    "        '''\n",
    "        ### Load and return the scvis embeddings\n",
    "        search_string = f'{self.config.scvis_output_dir}/*.tsv'\n",
    "        scvis_embedding_filepath = sorted(glob.glob(search_string), key = len)[0]\n",
    "        return pd.read_csv(scvis_embedding_filepath, sep = '\\t', index_col = 0).values\n",
    "    \n",
    "    def _combine_embedding(self, \n",
    "                           scvis_reps: np.ndarray, \n",
    "                           pred_probs: np.ndarray) -> np.ndarray:\n",
    "        ''' Normalizes \n",
    "        '''\n",
    "        # Normalize the embeddings using the minimum and maximum column values\n",
    "        X = np.copy(scvis_reps)\n",
    "        X -= self.min_scvis_vals\n",
    "        X /= self.max_scvis_vals\n",
    "        \n",
    "        # Append (weighted) predicted probabilities to the embedding\n",
    "        return np.concatenate((X, self.config.weight * (1. - pred_probs).reshape(-1, 1)), axis = 1)\n",
    "        \n",
    "    def _fit_scvis(\n",
    "        self, embeddings: np.ndarray\n",
    "    ):\n",
    "        ''' Fits an scvis model to the input embedding(s).\n",
    "        '''\n",
    "        if self.fit_scvis:\n",
    "            ### Fit scvis\n",
    "            \n",
    "            # Make output directory\n",
    "            os.system(f'rm -rf {self.config.scvis_output_dir}')\n",
    "            os.system(f'mkdir {self.config.scvis_output_dir}')\n",
    "\n",
    "            # Dump the embeddings as a CSV file\n",
    "            embedding_filepath = f'{self.config.scvis_output_dir}/tmp.tsv'\n",
    "            embedding_df = pd.DataFrame(embeddings)\n",
    "            embedding_df.to_csv(embedding_filepath, sep = '\\t', index = False)\n",
    "\n",
    "            # Run scvis using the command line\n",
    "            # source: https://github.com/shahcompbio/scvis\n",
    "            command = f'conda run -n {self.scvis_conda_env} scvis train --data_matrix_file {embedding_filepath} --out_dir {self.config.scvis_output_dir}'\n",
    "\n",
    "            if self.config.scvis_config_path is not None:\n",
    "                print(self.config.scvis_config_path)\n",
    "                # Add optional scvis config\n",
    "                command += f' --config_file {self.config.scvis_config_path}'\n",
    "\n",
    "            # Run the command (blocking)\n",
    "            print(command)\n",
    "            os.system(command)\n",
    "            print('done')\n",
    "\n",
    "            # Cleanup\n",
    "            os.system('rm -rf {}'.format(embedding_filepath))\n",
    "        \n",
    "        ### Load and return the scvis embeddings\n",
    "        return self._load_scvis_embeddings()\n",
    "    \n",
    "    def _fit_gmm(\n",
    "        self, \n",
    "        reduced_embeddings: np.ndarray, \n",
    "        pred_probs: np.ndarray,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        # Store the min and max column values to normalize in the future.\n",
    "        self.min_scvis_vals = np.min(reduced_embeddings, axis = 0)\n",
    "        self.max_scvis_vals = np.max(reduced_embeddings, axis = 0)\n",
    "\n",
    "        X = self._combine_embedding(reduced_embeddings, pred_probs)\n",
    "        \n",
    "        lowest_bic = np.infty\n",
    "        bic = []\n",
    "        n_components_range = range(self.config.n_slices, self.config.n_max_mixture_components)\n",
    "\n",
    "        for n_components in n_components_range:\n",
    "            # Fit a GMM with n_components components\n",
    "            gmm = mixture.GaussianMixture(n_components = n_components, covariance_type = 'full')\n",
    "            gmm.fit(X)\n",
    "            \n",
    "            # Calculate the Bayesian Information Criteria\n",
    "            bic.append(gmm.bic(X))\n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_gmm = gmm\n",
    "                \n",
    "        self.gmm = best_gmm\n",
    "        \n",
    "        # Assign a score to each mixture component to find the top-k slices\n",
    "        # Create the map from \"group\" to \"set of points\" (recorded as indices)\n",
    "        hard_preds = self.gmm.predict(X)\n",
    "        \n",
    "        cluster_map = defaultdict(list)\n",
    "        for i, v in enumerate(hard_preds):        \n",
    "            cluster_map[v].append(i)\n",
    "            \n",
    "        # Score each of those groups\n",
    "        scores = []\n",
    "        errors = (1. - pred_probs)\n",
    "        for i in cluster_map:\n",
    "            indices = cluster_map[i]\n",
    "            score = len(indices) * np.mean(errors[indices]) ** 2 # Equivalent to 'number of errors * error rate'\n",
    "            scores.append((i, score))\n",
    "        scores = sorted(scores, key = lambda x: -1 * x[1])\n",
    "        \n",
    "        print(scores)\n",
    "        print(verbose)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Scores:')\n",
    "            for i, score in scores:\n",
    "                indices = cluster_map[i]\n",
    "                print(i, score, len(indices) * np.mean(errors[indices]), np.mean(errors[indices]))\n",
    "            print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1fe01",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "(copied from `examples/01_intro.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b319004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dp = mk.datasets.get(\"imagenette\")\n",
    "\n",
    "# we'll only be using the validation data\n",
    "dp = dp.lz[dp[\"split\"] == \"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a400423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms as transforms\n",
    "model = resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5546f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b66c98182f647d9a26a686f06d6afee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 2. Create new column with transform \n",
    "dp[\"input\"] = dp[\"img\"].to_lambda(transform)\n",
    "\n",
    "# 1. Move the model to device\n",
    "DEVICE = 'cpu'\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "# 2. Define a function that runs a forward pass over a batch \n",
    "@torch.no_grad()\n",
    "def predict(batch: mk.DataPanel):\n",
    "    input_col: mk.TensorColumn = batch[\"input\"] \n",
    "    x: torch.Tensor = input_col.data.to(DEVICE)  # We get the underlying torch tensor with `data` and move to GPU \n",
    "    out: torch.Tensor = model(x)  # Run forward pass\n",
    "\n",
    "    # Return a dictionary with one key for each of the new columns. Each value in the\n",
    "    # dictionary should have the same length as the batch. \n",
    "    return {\n",
    "        \"pred\": out.cpu().numpy().argmax(axis=-1),\n",
    "        \"probs\": torch.softmax(out, axis=-1).cpu().numpy(),\n",
    "    }\n",
    "\n",
    "# 3. Apply the update. Note that the `predict` function operates on batches, so we set \n",
    "# `is_batched_fn=True`. Also, the `predict` function only accesses the \"input\" column, by \n",
    "# specifying that here we instruct update to only load that one column and skip others \n",
    "dp = dp.update(\n",
    "    function=predict,\n",
    "    is_batched_fn=True,\n",
    "    batch_size=32,\n",
    "    input_columns=[\"input\"], \n",
    "    pbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3706b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "import hashlib\n",
    "x = \"helo\"\n",
    "int.from_bytes(hashlib.sha256(x.encode('utf-8')).digest(), 'big') % 100\n",
    "\n",
    "dp[\"correct\"] = dp[\"pred\"] == mk.NumpyArrayColumn(dp[\"label_idx\"])\n",
    "accuracy = dp[\"correct\"].mean()\n",
    "print(f\"Micro accuracy across the ten Imagenette classes: {accuracy:0.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a single ImageNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee452a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_IDX = 571\n",
    "\n",
    "# convert to a binary task \n",
    "dp[\"prob\"] = dp[\"probs\"][:, LABEL_IDX]\n",
    "dp[\"target\"] = (dp[\"label_idx\"] == LABEL_IDX)\n",
    "\n",
    "# Drop rows that do not have the label\n",
    "dp = dp[dp['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba26af85",
   "metadata": {},
   "source": [
    "## 1. Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5cc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from domino import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features:\n",
    "    def __init__(self, requires_grad = None):\n",
    "        self.features = None\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "    def __call__(self, modules, module_in, module_out):\n",
    "        if self.requires_grad is not None:\n",
    "            module_out.requires_grad = self.requires_grad\n",
    "        self.features = module_out\n",
    "        \n",
    "# Register feature hook\n",
    "feature_hook = Features()\n",
    "handle = list(model.modules())[66].register_forward_hook(feature_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c9df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last-layer embeddings from the model, and add them as the \"embedding\" column.\n",
    "\n",
    "def last_layer(batch: mk.DataPanel):\n",
    "    input_col: mk.TensorColumn = batch[\"input\"] \n",
    "    x: torch.Tensor = input_col.data.to(DEVICE)  # We get the underlying torch tensor with `data` and move to GPU \n",
    "    \n",
    "    ## add a hook to the model\n",
    "    out: torch.Tensor = model(x)  # Run forward pass\n",
    "    features: np.ndarray = feature_hook.features.data.cpu().numpy()\n",
    "\n",
    "    # Return a dictionary with one key for each of the new columns. Each value in the\n",
    "    # dictionary should have the same length as the batch. \n",
    "    return {\n",
    "        \"embedding\": features\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = dp.update(\n",
    "    function=last_layer,\n",
    "    is_batched_fn=True,\n",
    "    batch_size=32,\n",
    "    input_columns=[\"input\"], \n",
    "    pbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aac199",
   "metadata": {},
   "source": [
    "# 2. Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55757ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "planespot = PlaneSpotSlicer(scvis_conda_env = 'scvis',\n",
    "                           fit_scvis = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cabc1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54146"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dp['prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "658cc65f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting scvis...\n",
      "Fitting GMM...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3925 and the array at index 1 has size 419",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rb/jmqwn68s2t3g0k5scvtpf1hm0000gr/T/ipykernel_5353/252787717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplanespot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prob\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/rb/jmqwn68s2t3g0k5scvtpf1hm0000gr/T/ipykernel_5353/211789704.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, embeddings, targets, pred_probs, losses, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting GMM...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         self._fit_gmm(scvis_embeddings,\n\u001b[0m\u001b[1;32m     92\u001b[0m                      \u001b[0mpred_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                      verbose)\n",
      "\u001b[0;32m/var/folders/rb/jmqwn68s2t3g0k5scvtpf1hm0000gr/T/ipykernel_5353/211789704.py\u001b[0m in \u001b[0;36m_fit_gmm\u001b[0;34m(self, reduced_embeddings, pred_probs, verbose)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_scvis_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mlowest_bic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rb/jmqwn68s2t3g0k5scvtpf1hm0000gr/T/ipykernel_5353/211789704.py\u001b[0m in \u001b[0;36m_combine_embedding\u001b[0;34m(self, scvis_reps, pred_probs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Append (weighted) predicted probabilities to the embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     def _fit_scvis(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3925 and the array at index 1 has size 419"
     ]
    }
   ],
   "source": [
    "planespot.fit(data = dp, embeddings=\"embedding\", targets=\"target\", pred_probs=\"prob\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae79ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp['scvis'] = planespot._load_scvis_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp[\"planespot_slices\"] = planespot.predict_proba(\n",
    "    data=dp, scvis_embeddings = 'scvis', pred_probs = 'prob'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b009d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d5f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdm-camera",
   "language": "python",
   "name": "bdm-camera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
